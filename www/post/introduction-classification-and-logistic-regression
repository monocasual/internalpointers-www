<!DOCTYPE html>
<html lang="en">
	<head>
		 
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23296419-22"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-23296419-22');
		</script>
		 

		<title>Introduction to classification and logistic regression - Internal Pointers</title>

		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Monocasual Laboratories">
		<meta name="description" content="Get your feet wet with another fundamental machine learning algorithm for binary classification.">
		<meta name="keywords" content="machine learning,logistic regression,classification,hypothesis function">
		<meta name="copyright" content="2015-2024 Monocasual Laboratories">
		<meta name="application-name" content="Internal Pointers">
		<meta name="google-site-verification" content="d6wzhBnnEXNHg7kty5SNXVBKd4e29wUFP69SROd-3eI" />

		<meta property="og:title" content="Introduction to classification and logistic regression" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.internalpointers.com/post/introduction-classification-and-logistic-regression" />
<meta property="og:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="900" />
<meta property="og:site_name" content="Internal Pointers" />
<meta property="og:description" content="Get your feet wet with another fundamental machine learning algorithm for binary classification." />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://www.internalpointers.com/post/introduction-classification-and-logistic-regression" />
<meta name="twitter:title" content="Introduction to classification and logistic regression" />
<meta name="twitter:description" content="Get your feet wet with another fundamental machine learning algorithm for binary classification." />
<meta name="twitter:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />

		<link rel="icon" href="/img/favicon.ico">
		<link rel="apple-touch-icon-precomposed" href="/img/favicon-152.png">
		<link rel="stylesheet" href="/main-1.4.0.css">

				
		<script defer src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
		<script defer src="//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js"></script>
		<script defer src="/main-1.4.0.js"></script>

		

<script defer type="text/x-mathjax-config">
	MathJax.Hub.Config({
		displayAlign: 'center',
		asciimath2jax: {
			delimiters: [['§','§']]
		},
		tex2jax: {
			inlineMath: [['[texi]','[texi]']],
			displayMath: [['[tex]','[tex]']]
		}
	});
</script>
<script defer type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
	</head>
	<body>
		<div class="ip-follow-us-popup">

    <div class="ip-follow-us-popup__side">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
    </div>

    <div class="ip-follow-us-popup__header">
        <p>Join us on Facebook!</p>
    </div>

    <div class="ip-follow-us-popup__body">
        <div class="ip-follow-us-popup__body__ok">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
        </div>
    </div>

    <div class="ip-follow-us-popup__footer">
        <div><a href="#" class="ip-follow-us-popup__footer__nope">Nope, thanks anyway.</a></div>
    </div>

</div>		<div class="ip-cookie-banner">
    <div class="ip-container">
        <p>We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. By using our site, you acknowledge that you have read and understand our <a href="{{ url('/privacy/') }}">Privacy Policy</a>, and our <a href="{{ url('/tos/') }}">Terms of Service</a>. Your use of this site is subject to these policies and terms. | <a href="#" class="ip-cookie-banner__close">ok, got it</a></p>
    </div>
</div>		<div class="ip-header">
	<div class="ip-container">
		<div class="ip-header__logo">
			<a href="/">**internal / pointers</a>
		</div>
		<div class="ip-header__links">
			<ul>
				<li><a href="/rss">rss</a></li>
				<li><a href="/about">about</a></li>
			</ul>
		</div>
	</div>
</div>

<div class="ip-sub-header">
</div>
		<div class="ip-body">

	<div class="ip-container">

		<div class="ip-post">

			<div class="ip-post__info">
				<p>— Written by Triangles on July 17, 2017 
								• updated on January 24, 2019  
								• ID 55 —</p>
			</div>

			<div class="ip-post__title">
				<h1>Introduction to classification and logistic regression</h1>
			</div>

			<div class="ip-post__intro">
				<p>Get your feet wet with another fundamental machine learning algorithm for binary classification.</p>
			</div>

						<div class="ip-post__other-box">
				<div class="ip-post__other-box__section-title">Other articles from this series</div>

				<ul class="ip-post__other-box__post-list">
														<li>
						<p>
							<span class="title">
								<a href="/post/introduction-machine-learning">Introduction to machine learning</a>
							</span> —
							<span class="intro">What machine learning is about, types of learning and classification algorithms, introductory examples.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/linear-regression-one-variable">Linear regression with one variable</a>
							</span> —
							<span class="intro">Finding the best-fitting straight line through points of a data set.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/gradient-descent-function">The gradient descent function</a>
							</span> —
							<span class="intro">How to find the minimum of a function using an iterative algorithm.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/gradient-descent-action">The gradient descent in action</a>
							</span> —
							<span class="intro">It's time to put together the gradient descent with the cost function, in order to churn out the final algorithm for linear regression.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/multivariate-linear-regression">Multivariate linear regression</a>
							</span> —
							<span class="intro">How to upgrade a linear regression algorithm from one to many input variables.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/optimize-gradient-descent-algorithm">How to optimize the gradient descent algorithm</a>
							</span> —
							<span class="intro">A collection of practical tips and tricks to improve the gradient descent process and make it easier to understand.</span>
						</p>
					</li>
																												<li>
						<p>
							<span class="title">
								<a href="/post/cost-function-logistic-regression">The cost function in logistic regression</a>
							</span> —
							<span class="intro">Preparing the logistic regression algorithm for the actual implementation.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/problem-overfitting-machine-learning-algorithms">The problem of overfitting in machine learning algorithms</a>
							</span> —
							<span class="intro">Overfitting makes linear regression and logistic regression perform poorly. A technique called "regularization" aims to fix the problem for good.</span>
						</p>
					</li>
													</ul>
			</div>
			
			<div class="ip-post__body">
				<p><strong>Classification</strong> is another big family of machine learning algorithms. Unlike <a href="https://www.internalpointers.com/post/linear-regression-one-variable">regression</a>, where the output can take on <em>continue</em> values, in classification your algorithms produce <em>discrete</em> outcomes: one/zero, yes/no, do/don't and so on.</p>
<p>Spam versus non-spam emails is a traditional example of classification task: you want to predict if the email fed to your program is spammy or not, where usually [texi]0[texi] means <em>not spam</em> and [texi]1[texi] means <em>spam</em>.</p>
<p>Formally, we want to predict a variable [texi]y \in \{0,1\}[texi], where [texi]0[texi] is called <strong>negative class</strong>, while [texi]1[texi] is called <strong>positive class</strong>. Such task is known as <strong>binary classification</strong>.</p>
<p>Other classification problems might require more than a binary output, for example where [texi]y \in \{0,1,2,3\}[texi]. Such task is known as a <strong>multiclass classification</strong>.</p>
<h2>Linear regression doesn't work with classification</h2>
<p>Let's start from how not to do things. In classification problems, <a href="https://www.internalpointers.com/post/linear-regression-one-variable">linear regression</a> performs very poorly and when it works it's usually a stroke of luck. The main reason is that in classification, unlike in regression, you don't have to choose the best line through a set of points, but rather you want to somehow <em>separate</em> those points.</p>
<p>Say for example that you are playing with image recognition: given a bunch of photos of bananas, you want to tell whether they are ripe or not, given the color. You collect the data and plot it as in the left picture in figure 1. below. Every white dot is an element in the training set; after the linear regression algorithm has been run on it, you end up with the well-known hypothesis function [texi]h_\theta(x)[texi] depicted by the dashed line.</p>
<p>You could now say that when [texi]h_\theta(x) \geq 0.5[texi] the outcome is [texi]1[texi] (ripe), and [texi]0[texi] otherwise, as marked by the empty circle on the hypothesis line. Everything appears to be in order: above a certain threshold of <em>yellowness</em>, the bananas are ripe, below they are not.</p>
<p>However, as more training examples are added (picture 1., right side), they might alter the original slope of the hypothesis line, thus distorting the threshold. That's the main reason why applying linear regression to classification problems often it's not a good idea.</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/06/classification-linear-regression-1.png" alt="Classification with linear regression">
<div class="caption">1. First attempt to handle a classification problem with linear regression. Outliers distort the hypothesis line.</div>
</div>

<h2>Logistic regression to the rescue</h2>
<p><strong>Logistic regression</strong> is a more performant algorithm used in classification problems. The most important feature is its ability to produce a sort of <em>hard-limited</em> hypothesis function: [texi]0 \leq h_\theta(x) \leq 1[texi].</p>
<p>The hypothesis function is slightly different from the one used in linear regression. For logistic regression,</p>
<p>[tex]h_\theta(x) = g(\theta^{\top} x)[tex]</p>
<p>which is the traditional hypothesis function processed by a new function [texi]g[texi], defined as:</p>
<p>[tex]g(z) = \frac{1}{1 + e^{-z}}[tex]</p>
<p>It is called <strong>sigmoid function</strong> or <strong>logistic function</strong> and looks like the picture 2.:</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/06/sigmoid-function-logistic-curve.png" alt="Sigmoid function">
<div class="caption">2. The sigmoid function. Credits: Wikipedia.</div>
</div>

<p>In a sigmoid function, as the input [texi]z[texi] goes to [texi]- \infty[texi], the output [texi]g(z)[texi] approaches to [texi]0[texi]; as [texi]z[texi] goes to [texi]+ \infty[texi], the output [texi]g(z)[texi] approaches to [texi]1[texi]. This works like an output limiter which makes the hypothesis function bound between [texi]0[texi] and [texi]1[texi].</p>

<p>For clarity let me unroll the hypothesis function plugged into the sigmoid:</p>
<p>[tex]h_\theta(x) = g(\theta^{\top} x)[tex]</p>
<p>becomes</p>
<p>[tex]h_\theta(x) = \frac{1}{1 + e^{-\theta^{\top} x}}[tex]</p>
<p>But what's the meaning of it?</p>
<h2>Interpretation of the new hypothesis output</h2>
<p>The logistic regression's hypothesis function outputs a number between [texi]0[texi] and [texi]1[texi]. You can think of it as the <strong>estimated probability</strong> that [texi]y = 1[texi] on a new example [texi]x[texi] in input.</p>
<p>Let's go back to the bananas classification task. It is a single-feature problem (i.e. the "yellowness"), so my feature vector [texi]x[texi] is defined as</p>
<p><span>
[tex]
\vec{x} =
\begin{bmatrix}
  x_0 \\ x_1
\end{bmatrix}
[tex]
</span></p>
<p>Where the first feature [texi]x_0 = 1[texi] is just a trick I've explained in <a href="https://www.internalpointers.com/post/multivariate-linear-regression">one of the previous articles</a> and [texi]x_1[texi] is the "yellowness" of each banana.</p>
<p>It's now time to define whether a new picture of a banana given in input to my algorithm is ripe or not. I grab its feature vector and plug it into the hypothesis function, which magically produces some outcome, say:</p>
<p>[tex]h_\theta(x) = 0.9[tex]</p>
<p>The hypothesis is telling me that for the new banana in input the probability that [texi]y = 1[texi] is [texi]0.9[texi]. In other words, the new banana in the picture has 90% chance of being ripe.</p>
<p>Let me write it more formally and generally:</p>
<p>[tex]h_\theta(x) = P(y = 1 | x; \theta)[tex]</p>
<p>In words, the hypothesis function tells you the probability that [texi]y = 1[texi] given [texi]x[texi] (i.e. given the new banana in input with a yellowness represented by the feature [texi]x[texi]), parametrized by [texi]\theta[texi] (i.e. whose parameters are [texi]\theta[texi]).</p>
<p>Since the outcome [texi]y[texi] is restricted between two values [texi]0[texi] and [texi]1[texi], I can compute the probability that [texi]y = 0[texi] as well. Let's figure out the following statement:</p>
<p>[tex]P(y = 0 | x; \theta) + P(y = 1 | x; \theta) = 1[tex]</p>
<p>The sum of each probability, namely of [texi]y[texi] being [texi]0[texi] and [texi]y[texi] being [texi]1[texi] is of course [texi]1[texi] (or 100%). So by moving one term on the other side of the equation, we will end up with:</p>
<p>[tex]P(y = 0 | x; \theta) = 1 - P(y = 1 | x; \theta)[tex]</p>
<p>Back to the banana example, we had an estimated probability of [texi]0.9[texi], or:</p>
<p>[tex]h_\theta(x) = P(y = 1 | x; \theta) = 0.9[tex]</p>
<p>Let's find the probability of [texi]y[texi] being [texi]0[texi], that is an unripe banana:</p>
<p>[tex]P(y = 0 | x; \theta) = 1 - 0.9 = 0.1[tex]</p>
<p>In other words, the new banana has 10% chance of being unripe.</p>
<h2>Defining the binary output</h2>
<p>So far I've talked about percentages: where is the binary output? It's very simple to obtain. We already know that the hypothesis function [texi]h_\theta(x) = g(\theta^{\top} x)[texi] churns out values in range [texi][0 ,1][texi]: let's make the following assumption:</p>
<ul>
<li>if [texi]h_\theta(x) \geq 0.5[texi] predict [texi]y = 1[texi]</li>
<li>if [texi]h_\theta(x) &lt; 0.5[texi] predict [texi]y = 0[texi]</li>
</ul>
<p>That's kind of intuitive. Now, when exactly is <span>[texi]h_\theta(x)[texi]</span> above or below [texi]0.5[texi]? If you look at the generic sigmoid function above (picture 2.), you'll notice that [texi]g(z) \geq 0.5 [texi] whenever [texi]z \geq 0[texi]. Since the hypothesis function for logistic regression is defined as <span>[texi]h_\theta(x) = g(\theta^{\top} x)[texi]</span>, I can clearly state that <span>[texi]h_\theta(x) = g(\theta^{\top} x) \geq 0.5[texi]</span> whenever <span>[texi]\theta^{\top} x \geq 0[texi]</span> (because <span>[texi]z = \theta^{\top} x[texi]</span>, right?).</p>
<p>You can easily figure out when [texi]g(z) &lt; 0.5 [texi]. I can now update the assumptions above:</p>
<ul>
<li>if [texi]h_\theta(x) \geq 0.5[texi] (i.e. [texi]\theta^{\top} x \geq 0[texi]) predict [texi]y = 1[texi]</li>
<li>if [texi]h_\theta(x) &lt; 0.5[texi] (i.e. [texi]\theta^{\top} x &lt; 0[texi]) predict [texi]y = 0[texi]</li>
</ul>
<h2>The decision boundary</h2>
<p>Let's now glue together all those pieces and take a look at a graphical example of logistic regression. Suppose you have a training set as in picture 3. below. Two features [texi]x_1[texi] and [texi]x_2[texi] — size and yellowness of a banana for example — and a bunch of items scattered around the plane — each point is a banana in the training set, where empy points represent unripe bananas.</p>
<p>The task of the logistic regression algorithm is to <em>separate</em> those points, by drawing a line between them. That line is technically called <strong>decision boundary</strong> and it is used to infer about the data set: everything that is <em>below</em> the decision boundary belongs to category A and the remaining part belongs to category B.</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/06/decision-boundary.png" alt="Decision boundary">
<div class="caption">3. A simple logistic regression problem with two features and the decision boundary depicted as the dotted line.</div>
</div>

<p>The decision boundary is a line, hence it can be described by an equation. As in linear regression, the logistic regression algorithm will be able to find the best [texi]\theta[texi]s parameters in order to make the decision boundary actually separate the data points correctly.</p>
<p>We still don't know how to compute those parameters — I will talk about it in the next chapter. For now suppose that we know the hypothesis function:</p>
<p>[tex]
h_\theta(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)
[tex]</p>
<p>and through some magical procedures we end up with the following parameters:</p>
<p><span>
[tex]
\begin{align}
\theta_0 & = -3 \\
\theta_1 & = 1  \\
\theta_2 & = 1
\end{align}
[tex]
</span></p>
<p>which form the usual parameter vector like:</p>
<p><span>
[tex]
\vec{\theta} =
\begin{bmatrix}
  -3 \\ 1 \\ 1
\end{bmatrix}
[tex]
</span></p>
<p>We know since the beginning that [texi]h_\theta(x) = g(\theta^{\top} x)[texi] and for the current example [texi]\theta^{\top} x = \theta_0 + \theta_1x_1 + \theta_2x_2[texi] which is, not surpisingly, the equation of a line (the decision boundary). We also know from the previous paragraph that we can predict [texi]y = 1[texi] whenever [texi]\theta^{\top} x \geq 0[texi]. Thus can state that:</p>
<p><span>
[tex]
y = 1 \ \ \text{if} \ \ \theta_0 + \theta_1x_1 + \theta_2x_2 \geq 0
[tex]
</span></p>
<p>And for our specific example:</p>
<p><span>
[tex]
\begin{align}
& y = 1 \ \ \text{if} \ \ \ -3 + x_1 + x_2 \geq 0 \\
& y = 1 \ \ \text{if} \ \ \ x_1 + x_2 \geq 3
\end{align}
[tex]
</span></p>
<p>If you draw the equation [texi]x_1 + x_2 = 3[texi] you will obtain the decision boundary line in picture 3. above.</p>
<p>In words: given the current training set, pick a new example from the real world with features [texi]x_1,\ x_2[texi] — a banana with a certain level of yellowness and a certain size. It will be classified as 1 ([texi]y = 1[texi] i.e. ripe) whenever it satisfies the inequation [texi]x_1 + x_2 \geq 3[texi], that is whenever it lies <em>above</em> ([texi]\geq[texi]) the decision boundary. It will be classified as 0 ([texi]y = 0[texi] i.e. unripe) otherwise.</p>
<p>Here the power of the sigmoid function emerges. It does not matter how far the new example lies from the decision boundary: [texi]y = 1[texi] if above; else [texi]y = 0[texi].</p>
<h3>How do I find the equation for the decision boundary?</h3>
<p>I made up the previous example: I already knew the shape of the decision boundary (a line) and its equation. In the real world you will find oddly-shaped decision boundaries, as a simple straight line doesn't always separate things well. Those shapes are usually described by polynomial equations.</p>
<p>Logistic regression has no built-in ability to define the decision boundary's equation. Usually it's good practice to look at data and figure out the appropriate shape to implement: that's the so-called <em>data exploration</em>. The <strong>gradient boosted logistic regression</strong>, an advanced machine learning algorithm, has the ability to generate the decision boundary by itself. I will look into it in future sessions.</p>
<h2>Sources</h2>
<p>Machine Learning Course @ Coursera - <em>Classification</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification">link</a>)<br>
Machine Learning Course @ Coursera - <em>Hypothesis representation</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/RJXfB/hypothesis-representation">link</a>)<br>
Machine Learning Course @ Coursera - <em>Decision boundary</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/WuL1H/decision-boundary">link</a>)<br>
Cross Validated - <em>How is the decision boundary's equation determined?</em> (<a href="https://stats.stackexchange.com/questions/289503/how-is-the-decision-boundarys-equation-determined">link</a>)</p>			</div>


			<div class="ip-post__tags">
								<a class="ip-tag" href="/tag/machine-learning">machine learning</a>
				 • 								<a class="ip-tag" href="/tag/logistic-regression">logistic regression</a>
				 • 								<a class="ip-tag" href="/tag/classification">classification</a>
				 • 								<a class="ip-tag" href="/tag/hypothesis-function">hypothesis function</a>
											</div>

			<div class="ip-post__neighbor-posts">
								<div class="ip-post__neighbor-posts__prev">
					<div class="ip-post__neighbor-posts__prev__label">
						previous article
					</div>
					<div class="ip-post__neighbor-posts__prev__title">          
						<a href="/post/optimize-gradient-descent-algorithm">How to optimize the gradient descent algorithm</a>
					</div>
				</div>
								
								<div class="ip-post__neighbor-posts__next">
					<div class="ip-post__neighbor-posts__next__label">
						next article
					</div>
					<div class="ip-post__neighbor-posts__next__title">          
						<a href="/post/cost-function-logistic-regression">The cost function in logistic regression</a>
					</div>
				</div>
							</div>
			
						<div class="ip-post__comments">
				<div class="ip-post__comments__title">
					comments
				</div>
				
								<div class="ip-post__comments__list">
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">TooTiredOne</span> on 
							<span class="date">January 23, 2019 at 19:36</span>
						</div> 
						<div class="body">Thank you very much for this masterpiece in the world of machine learning!<br />
I really enjoyed reading the article as the explanation is clear as the soul of a newborn child, the language is nice, without any mindblowing math words, and even the design of the page is amazing! You did a great job and made my life much easier!<br />
<br />
Super tiny but: it seems to me that you missed the minus sign in the formula of the hypothesis function in the "Logistic regression to the rescue" section :p</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Triangles</span> on 
							<span class="date">January 24, 2019 at 10:13</span>
						</div> 
						<div class="body">Thank you @TooTiredOne for your kind words and precious feedback! I've fixed the glitch in the formula :)</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Lloyd</span> on 
							<span class="date">November 30, 2019 at 17:39</span>
						</div> 
						<div class="body">this awesome,  its self explanatory and I thank you... do you have its implementation in python?</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Triangles</span> on 
							<span class="date">December 04, 2019 at 14:02</span>
						</div> 
						<div class="body">@Lloyd thanks! Unfortunately no Python implementation so far. </div> 
					</div>
									</div>
							</div>
			
			<div class="ip-post__social-tools">
				<div class="ip-post__social-tools__title">
					share!
				</div>
				<div class="ip-post__social-tools__twitter item"></div>
			</div>

		</div>

	</div>

</div>

		<div class="ip-footer">
	<div class="ip-container">
		© 2015-2024 — Monocasual Laboratories — 
		<a href="/tos" rel="nofollow">terms of service</a> — 
		<a href="/privacy" rel="nofollow">privacy policy</a> — 
		<a href="/about">about</a> — 
		<a href="/rss">RSS feed</a>
	</div>
</div>
	</body>
</html>

