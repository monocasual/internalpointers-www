<!DOCTYPE html>
<html lang="en">
	<head>
		 
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23296419-22"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-23296419-22');
		</script>
		 

		<title>The gradient descent function - Internal Pointers</title>

		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Monocasual Laboratories">
		<meta name="description" content="How to find the minimum of a function using an iterative algorithm.">
		<meta name="keywords" content="machine learning,gradient descent,function,learning rate">
		<meta name="copyright" content="2015-2024 Monocasual Laboratories">
		<meta name="application-name" content="Internal Pointers">
		<meta name="google-site-verification" content="d6wzhBnnEXNHg7kty5SNXVBKd4e29wUFP69SROd-3eI" />

		<meta property="og:title" content="The gradient descent function" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.internalpointers.com/post/gradient-descent-function" />
<meta property="og:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="900" />
<meta property="og:site_name" content="Internal Pointers" />
<meta property="og:description" content="How to find the minimum of a function using an iterative algorithm." />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://www.internalpointers.com/post/gradient-descent-function" />
<meta name="twitter:title" content="The gradient descent function" />
<meta name="twitter:description" content="How to find the minimum of a function using an iterative algorithm." />
<meta name="twitter:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />

		<link rel="icon" href="/img/favicon.ico">
		<link rel="apple-touch-icon-precomposed" href="/img/favicon-152.png">
		<link rel="stylesheet" href="/main-1.4.0.css">

				
		<script defer src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
		<script defer src="//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js"></script>
		<script defer src="/main-1.4.0.js"></script>

		

<script defer type="text/x-mathjax-config">
	MathJax.Hub.Config({
		displayAlign: 'center',
		asciimath2jax: {
			delimiters: [['§','§']]
		},
		tex2jax: {
			inlineMath: [['[texi]','[texi]']],
			displayMath: [['[tex]','[tex]']]
		}
	});
</script>
<script defer type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
	</head>
	<body>
		<div class="ip-follow-us-popup">

    <div class="ip-follow-us-popup__side">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
    </div>

    <div class="ip-follow-us-popup__header">
        <p>Join us on Facebook!</p>
    </div>

    <div class="ip-follow-us-popup__body">
        <div class="ip-follow-us-popup__body__ok">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
        </div>
    </div>

    <div class="ip-follow-us-popup__footer">
        <div><a href="#" class="ip-follow-us-popup__footer__nope">Nope, thanks anyway.</a></div>
    </div>

</div>		<div class="ip-cookie-banner">
    <div class="ip-container">
        <p>We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. By using our site, you acknowledge that you have read and understand our <a href="{{ url('/privacy/') }}">Privacy Policy</a>, and our <a href="{{ url('/tos/') }}">Terms of Service</a>. Your use of this site is subject to these policies and terms. | <a href="#" class="ip-cookie-banner__close">ok, got it</a></p>
    </div>
</div>		<div class="ip-header">
	<div class="ip-container">
		<div class="ip-header__logo">
			<a href="/">**internal / pointers</a>
		</div>
		<div class="ip-header__links">
			<ul>
				<li><a href="/rss">rss</a></li>
				<li><a href="/about">about</a></li>
			</ul>
		</div>
	</div>
</div>

<div class="ip-sub-header">
</div>
		<div class="ip-body">

	<div class="ip-container">

		<div class="ip-post">

			<div class="ip-post__info">
				<p>— Written by Triangles on February 05, 2017 
								• updated on December 04, 2019  
								• ID 48 —</p>
			</div>

			<div class="ip-post__title">
				<h1>The gradient descent function</h1>
			</div>

			<div class="ip-post__intro">
				<p>How to find the minimum of a function using an iterative algorithm.</p>
			</div>

						<div class="ip-post__other-box">
				<div class="ip-post__other-box__section-title">Other articles from this series</div>

				<ul class="ip-post__other-box__post-list">
														<li>
						<p>
							<span class="title">
								<a href="/post/introduction-machine-learning.html">Introduction to machine learning</a>
							</span> —
							<span class="intro">What machine learning is about, types of learning and classification algorithms, introductory examples.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/linear-regression-one-variable.html">Linear regression with one variable</a>
							</span> —
							<span class="intro">Finding the best-fitting straight line through points of a data set.</span>
						</p>
					</li>
																												<li>
						<p>
							<span class="title">
								<a href="/post/gradient-descent-action.html">The gradient descent in action</a>
							</span> —
							<span class="intro">It's time to put together the gradient descent with the cost function, in order to churn out the final algorithm for linear regression.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/multivariate-linear-regression.html">Multivariate linear regression</a>
							</span> —
							<span class="intro">How to upgrade a linear regression algorithm from one to many input variables.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/optimize-gradient-descent-algorithm.html">How to optimize the gradient descent algorithm</a>
							</span> —
							<span class="intro">A collection of practical tips and tricks to improve the gradient descent process and make it easier to understand.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/introduction-classification-and-logistic-regression.html">Introduction to classification and logistic regression</a>
							</span> —
							<span class="intro">Get your feet wet with another fundamental machine learning algorithm for binary classification.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/cost-function-logistic-regression.html">The cost function in logistic regression</a>
							</span> —
							<span class="intro">Preparing the logistic regression algorithm for the actual implementation.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/problem-overfitting-machine-learning-algorithms.html">The problem of overfitting in machine learning algorithms</a>
							</span> —
							<span class="intro">Overfitting makes linear regression and logistic regression perform poorly. A technique called "regularization" aims to fix the problem for good.</span>
						</p>
					</li>
													</ul>
			</div>
			
			<div class="ip-post__body">
				<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- internalpointers responsive -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1778432007040046"
     data-ad-slot="1269254897"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<p>During <a href="https://www.internalpointers.com/post/linear-regression-one-variable">the previous episode</a> of this Machine Learning series we took a look at the theory behind linear regression. We also outlined some paper formulas for <em>minimizing</em> the cost function <span>[texi]J[texi]</span>. Or, in other words, finding the best values of <span>[texi]\theta[texi]</span>'s, the parameters of the hypothesis function.</p>
<p>During that stage we basically guessed those best <span>[texi]\theta[texi]</span>'s by staring at the graphical plot on the screen. That was definitely not an automated task: an algorithm called <strong>gradient descent</strong> will come in handy.</p>
<p>Gradient descent is a more generic algorithm, used not only in linear regression problems and cost functions. In the following example I will minimize an arbitrary function <span>[texi]J[texi]</span>, then in the next chapter I'll apply it to the original house pricing task.</p>

<h2>The gradient descent algorithm in a nutshell</h2>
<p>You have a generic function <span>[texi]J(\theta_0, \theta_1, ... \theta_n)[texi]</span> with an undefined number of parameters. You want to minimize it, that is</p>
<p>
[tex]\displaystyle{\stackrel{\text{minimize}}{{\theta_{{0}},\theta_{{1}},\ldots\theta_{{n}}}}}\ {J}{\left(\theta_{{0}},\theta_{{1}},\ldots\theta_{{n}}\right)}[tex]
</p>
<p>What to do:</p>
<ol>
<li><p>start with some initial guesses for <span>[texi]\theta_0, \theta_1, ... \theta_n[texi]</span>. The common choice is to set them to <span>[texi]0[texi]</span> but sometimes you want to initialize them to other values as well;</p>
</li>
<li><p>keep changing those <span>[texi]\theta_0, \theta_1, ... \theta_n[texi]</span> values to try to reduce <span>[texi]J[texi]</span> until you find a <em>minimum</em> for that function.</p>
</li>
</ol>
<p>The picture 1. below displays a generic three-dimensional function <span>[texi]J(\theta_0, \theta_1)[texi]</span>. There are only two <span>[texi]\theta[texi]</span>'s there, in order to generate an understandable plot. The height, or the vertical axis shows <span>[texi]J[texi]</span>, that is the output. Minimizing that function means to find the lowest values of <span>[texi]J[texi]</span> that correspond to the blue depressed areas.</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/01/cost-function-3d.jpg" alt="A generic 3-dimensional function.">
<div class="caption">1. A generic, 3-dimensional function <span>[texi]J[texi]</span>.</div>
</div>

<h2>Applying the descent algorithm in the real world</h2>
<p>Suppose that the function in picture 1. represents a hilly landscape with yourself standing on the central mountain. Your task is to walk downhill as quickly as possible by taking small baby steps until you cannot go down any further. You start looking around you, asking yourself: what direction should I go in order to take a baby step downhill? You find the best one given your current position, you take the step and then ask yourself the same question again. Repeat the process until you cannot go down any further, that is until you find a minimum.</p>
<p>Picture 2. below shows the baby steps experiment with two different routes. Your starting position on the hill corresponds to the initial values given to <span>[texi]\theta_0, \theta_1[texi]</span>. Black route has a slightly different starting point compared to the white one, which reveals an interesting property of the gradient descent algorithm: changing the initial value of theta's might lead you to a different minimum.</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/01/gradient-descent-steps.jpg
" alt="Gradient descent in a three-dimensional function.">
<div class="caption">2. Two different downhill routes in a three-dimensional function, determined by the starting point.</div>
</div>

<h2>Gradient descent algorithm definition</h2>
<p>The complete gradient descent formula looks like the following:</p>
<p>
[tex]\displaystyle\theta_{{j}}:=\theta_{{j}}-\alpha\cdot\frac{\partial}{{\partial\theta_{{j}}}}{J}{\left(\theta_{{0}},\theta_{{1}},\ldots\theta_{{n}}\right)}\ \ \ \ \ \ \ \text{for}\ {j}={0},{j}={1},\ldots{j}={n}\ \text{until convergence}[tex]
</p>
<p>Let me dissect it a little. First of all the <span>[texi]:=[texi]</span> symbol means <em>assignment</em>. It works like when you assign a value to a variable in a programming language, for example <code>int x = 3</code>.</p>

<p>The term <span>[texi]\alpha[texi]</span> is a number called <strong>learning rate</strong>. It basically controls the step size while descending the hill. Larger <span>[texi]\alpha[texi]</span> means larger steps. The remaining part is a derivative of function <span>[texi]J[texi]</span>; I will delve into it shortly.</p>
<p>To put it briefly, the gradient descent works as follows: for every <span>[texi]\theta_j[texi]</span> of the <span>[texi]J[texi]</span> function (that is <span>[texi]\theta_0, \theta_1, ... \theta_n[texi]</span>), compute the value of <span>[texi]\theta_j[texi]</span> by subtracting from itself the derivative of the function at point <span>[texi]\theta_j[texi]</span> mupliplied by a number <span>[texi]\alpha[texi]</span>. Rinse and repeat <em>until convergence</em>. We will see the concept of convergence later. For now think of it as a point in your downhill walk in which you haven't reached the optimal result yet, but you are really quite quite near, if not on it.</p>
<p>The correct way to implement a gradient descent algorithm requires simultaneous updates of <span>[texi]\theta_j[texi]</span>. That is, store each value in a temporary variable:</p>
<p>
[tex]\displaystyle\text{temp}_{{0}}=\theta_{{0}}-\alpha\cdot\frac{\partial}{{\partial\theta_{{0}}}}{J}{\left(\theta_{{0}},\theta_{{1}},\ldots\theta_{{n}}\right)}[tex]
</p>
<p>
[tex]\displaystyle\text{temp}_{{1}}=\theta_{{1}}-\alpha\cdot\frac{\partial}{{\partial\theta_{{1}}}}{J}{\left(\theta_{{0}},\theta_{{1}},\ldots\theta_{{n}}\right)}[tex]
</p>
<p>
[tex]\displaystyle\text{...}[tex]
</p>
<p>
[tex]\displaystyle\text{temp}_{{n}}=\theta_{{n}}-\alpha\cdot\frac{\partial}{{\partial\theta_{{n}}}}{J}{\left(\theta_{{0}},\theta_{{1}},\ldots\theta_{{n}}\right)}[tex]
</p>
<p>Then assign those temporary values to the actual <span>[texi]\theta_j[texi]</span>:</p>
<p>
[tex]\displaystyle\theta_{{0}}=\text{temp}_{{0}}[tex]
</p>
<p>
[tex]\displaystyle\theta_{{1}}=\text{temp}_{{1}}[tex]
</p>
<p>
[tex]\displaystyle\text{...}[tex]
</p>
<p>
[tex]\displaystyle\theta_{{n}}=\text{temp}_{{n}}[tex]
</p>

<h2>A gradient descent algorithm at work</h2>
<p>Let's try now to understand what each part of the gradient descent algorithm does. I'm going to use a simpler example with only <em>one</em> parameter <span>[texi]\theta_0[texi]</span>: it will help a lot with visualization.</p>
<p>I have a generic function <span>[texi]J(\theta_0)[texi]</span> to minimize, that is <span>[texi]\displaystyle{\stackrel{\text{min}}{{\theta_{{0}}}}}\ {J}{\left(\theta_{{0}}\right)}[texi]</span>, with <span>[texi]\displaystyle\theta_{{0}}\in\mathbb{R}[texi]</span> (a real number). The function might look like the one you see in the picture 3. below.</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/01/parabola-function-shift-1.png
" alt="Generic parabola function with shifted points.">
<div class="caption">3. A generic function <span>[texi]J(\theta_0)[texi]</span> that looks like a parabola. On the left, a randomly choosen point <span>[texi]\theta_0[texi]</span> before the gradient descent. On the right, the point is closer to the minimum after an algorithm iteration.</div>

</div><p>You may also notice the black point on the curve: that's the initial value of <span>[texi]\theta_0[texi]</span> set during the gradient descent initialization. It's just a random value. The gradient descent function will shift that point until it reaches the minimum, that is the bottom of the parabola. Let's see how.</p>
<p>The core part of the algorithm is the derivative: it basically churns out the slope of the tangent line to the black point. Once the value of the slope is known, the algorithm adds or subtracts that value in order to move the point closer to the minimum.</p>
<p>In the picture 3. the slope of the tangent line is positive, so its derivative will be positive. The derivative at a point is just a number, let's call it <span>[texi]\displaystyle{d},\ {d}\ge{0}[texi]</span>. Then the update to <span>[texi]\theta_0[texi]</span> will be:</p>
<p>
[tex]\displaystyle\theta_{{0}}\:=\theta_{{0}}-\alpha\cdot{d}[tex]
</p>
<p>We are subtracting a positive number from <span>[texi]\theta_0[texi]</span>, which makes it smaller. The point will shift to the left, as you may see in the rightmost plot in picture 3.</p>
<p>Of course the algorithm works also in case you have a tangent with a negative slope, like when you initialize <span>[texi]\theta_0[texi]</span> so that it lands in the leftmost part of the plot. Look at the picture below for an example:</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/01/parabola-function-shift-2.png
" alt="Generic parabola function with shifted points.">
<div class="caption">4. The same function <span>[texi]J(\theta_0)[texi]</span> seen before, with a different starting point. Here the tangent line has negative slope.</div>
</div>

<p>Up there the slope of the tangent line is negative, so its derivative will be negative. Let's call it <span>[texi]\displaystyle{d},\ {d}\le{0}[texi]</span> as before. Then the update to <span>[texi]\theta_0[texi]</span> will be:</p>
<p>
[tex]\displaystyle\theta_{{0}}\:=\theta_{{0}}-\alpha\cdot{\left(-{d}\right)}[tex]
</p>
<p>that is</p>
<p>
[tex]\displaystyle\theta_{{0}}\:=\theta_{{0}}+\alpha\cdot{d}[tex]
</p>
<p>We are adding a positive number to <span>[texi]\theta_0[texi]</span>, which makes it larger. The point will shift to the right, as you may see in the rightmost plot in picture 4.</p>
<p>The process continues until the algorithm has reached the minimum. But what does it mean? The minimum, namely the bottom of the parabola, is the point where the tangent has slope <em>zero</em>: a perfectly horizontal line. The derivative of such horizontal line is <span>[texi]0[texi]</span> (let's call it <span>[texi]\displaystyle{d},\ {d}={0}[texi]</span>), so the algorithm will eventually add nothing to <span>[texi]\theta_0[texi]</span>, like so:</p>
<p>
[tex]\displaystyle\theta_{{0}}\:=\theta_{{0}}-\alpha\cdot{d}[tex]
</p>
<p>that is</p>
<p>
[tex]\displaystyle\theta_{{0}}\:=\theta_{{0}}-{0}[tex]
</p>
<p>At this point, <span>[texi]\theta_0[texi]</span> does not shift anymore: the minimum has been reached.</p>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- internalpointers responsive -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1778432007040046"
     data-ad-slot="1269254897"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h2>The gotchas of <span>[texi]\alpha[texi]</span>, the learning rate</h2>
<p>The parameter <span>[texi]\alpha[texi]</span> (the learning rate) defines how big the step will be during a gradient descent. It's the number that multiplies the derivative during the <span>[texi]\theta_0[texi]</span> updates. The greater the learning rate, the faster the algorithm will descent to the minimum point.</p>
<p>Defining a good value for <span>[texi]\alpha[texi]</span> requires some planning. If it's too small, the algorithm will take many tiny baby steps to get to the minimum, as shown in picture 5. below, left side. Thus gradient descent can be slow.</p>
<p>If <span>[texi]\alpha[texi]</span> is too large, the algorithm can miss the minimum point, namely it fails to converge, as seen in picture below, right side. Worse, it could <em>diverge</em>, that is going further and further away from the minimum, taking up an infinite amount of time.</p>

<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/01/alpha-step-size-gradient-descent.png
" alt="Alpha, or the step size in gradient descent.">
<div class="caption">5. The effects of different sizes for the learning rate <span>[texi]\alpha[texi]</span>. Baby steps on the left (slow processing), failure to converge on the right (divergence).</div>
</div>

<p>The good news is that once you've found a proper value for the learning rate, the algorithm will reach the minimum naturally, step by step. As <span>[texi]\theta_0[texi]</span> slides toward the minimum, the slope of the tangent line keeps getting smaller, until it reaches <span>[texi]0[texi]</span>. So there's no need to adjust <span>[texi]\alpha[texi]</span>, which can remain fixed for the entire process.</p>
<p>In the next episode I will try to apply the gradient descent algorithm to our initial house pricing and churn out some useful values from it.</p>

<h2>Sources</h2>
<p>StackOverflow - <em>Gradient descent convergence How to decide convergence?</em> (<a href="http://stackoverflow.com/questions/17289082/gradient-descent-convergence-how-to-decide-convergence">link</a>)<br>
Machine Learning @ Coursera - <em>Gradient Descent</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent">link</a>)<br>
Machine Learning @ Coursera - <em>Gradient Descent Intuition</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/GFFPB/gradient-descent-intuition">link</a>)</p>			</div>


			<div class="ip-post__tags">
								<a class="ip-tag" href="/tag/machine-learning">machine learning</a>
				 • 								<a class="ip-tag" href="/tag/gradient-descent">gradient descent</a>
				 • 								<a class="ip-tag" href="/tag/function">function</a>
				 • 								<a class="ip-tag" href="/tag/learning-rate">learning rate</a>
											</div>

			<div class="ip-post__neighbor-posts">
								<div class="ip-post__neighbor-posts__prev">
					<div class="ip-post__neighbor-posts__prev__label">
						previous article
					</div>
					<div class="ip-post__neighbor-posts__prev__title">          
						<a href="/post/linear-regression-one-variable.html">Linear regression with one variable</a>
					</div>
				</div>
								
								<div class="ip-post__neighbor-posts__next">
					<div class="ip-post__neighbor-posts__next__label">
						next article
					</div>
					<div class="ip-post__neighbor-posts__next__title">          
						<a href="/post/gradient-descent-action.html">The gradient descent in action</a>
					</div>
				</div>
							</div>
			
						<div class="ip-post__comments">
				<div class="ip-post__comments__title">
					comments
				</div>
				
								<div class="ip-post__comments__list">
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Spora23</span> on 
							<span class="date">February 24, 2018 at 19:41</span>
						</div> 
						<div class="body">Thanks, thanks, thanks. Finally an adequate introduction to the gradient descent formula.</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">KetanK</span> on 
							<span class="date">February 06, 2019 at 20:39</span>
						</div> 
						<div class="body">Awesome article mate. Really Helpful. Many thanks.</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Fred</span> on 
							<span class="date">December 02, 2019 at 02:10</span>
						</div> 
						<div class="body">Shouldn't the step in theta be smaller in size if the gradient is larger?  It seems to me that a steeper gradient means a small change in theta yields a huge change in "J".</div> 
					</div>
									</div>
							</div>
			
			<div class="ip-post__social-tools">
				<div class="ip-post__social-tools__title">
					share!
				</div>
				<div class="ip-post__social-tools__twitter item"></div>
			</div>

		</div>

	</div>

</div>

		<div class="ip-footer">
	<div class="ip-container">
		© 2015-2024 — Monocasual Laboratories — 
		<a href="/tos" rel="nofollow">terms of service</a> — 
		<a href="/privacy" rel="nofollow">privacy policy</a> — 
		<a href="/about">about</a> — 
		<a href="/rss">RSS feed</a>
	</div>
</div>
	</body>
</html>

