<!DOCTYPE html>
<html lang="en">
	<head>
		 
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23296419-22"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-23296419-22');
		</script>
		 

		<title>The cost function in logistic regression - Internal Pointers</title>

		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Monocasual Laboratories">
		<meta name="description" content="Preparing the logistic regression algorithm for the actual implementation.">
		<meta name="keywords" content="machine learning,logistic regression,classification,hypothesis function,cost function">
		<meta name="copyright" content="2015-2024 Monocasual Laboratories">
		<meta name="application-name" content="Internal Pointers">
		<meta name="google-site-verification" content="d6wzhBnnEXNHg7kty5SNXVBKd4e29wUFP69SROd-3eI" />

		<meta property="og:title" content="The cost function in logistic regression" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.internalpointers.com/post/cost-function-logistic-regression" />
<meta property="og:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="900" />
<meta property="og:site_name" content="Internal Pointers" />
<meta property="og:description" content="Preparing the logistic regression algorithm for the actual implementation." />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://www.internalpointers.com/post/cost-function-logistic-regression" />
<meta name="twitter:title" content="The cost function in logistic regression" />
<meta name="twitter:description" content="Preparing the logistic regression algorithm for the actual implementation." />
<meta name="twitter:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />

		<link rel="icon" href="/img/favicon.ico">
		<link rel="apple-touch-icon-precomposed" href="/img/favicon-152.png">
		<link rel="stylesheet" href="/main-1.4.0.css">

				
		<script defer src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
		<script defer src="//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js"></script>
		<script defer src="/main-1.4.0.js"></script>

		

<script defer type="text/x-mathjax-config">
	MathJax.Hub.Config({
		displayAlign: 'center',
		asciimath2jax: {
			delimiters: [['§','§']]
		},
		tex2jax: {
			inlineMath: [['[texi]','[texi]']],
			displayMath: [['[tex]','[tex]']]
		}
	});
</script>
<script defer type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
	</head>
	<body>
		<div class="ip-follow-us-popup">

    <div class="ip-follow-us-popup__side">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
    </div>

    <div class="ip-follow-us-popup__header">
        <p>Join us on Facebook!</p>
    </div>

    <div class="ip-follow-us-popup__body">
        <div class="ip-follow-us-popup__body__ok">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
        </div>
    </div>

    <div class="ip-follow-us-popup__footer">
        <div><a href="#" class="ip-follow-us-popup__footer__nope">Nope, thanks anyway.</a></div>
    </div>

</div>		<div class="ip-cookie-banner">
    <div class="ip-container">
        <p>We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. By using our site, you acknowledge that you have read and understand our <a href="{{ url('/privacy/') }}">Privacy Policy</a>, and our <a href="{{ url('/tos/') }}">Terms of Service</a>. Your use of this site is subject to these policies and terms. | <a href="#" class="ip-cookie-banner__close">ok, got it</a></p>
    </div>
</div>		<div class="ip-header">
	<div class="ip-container">
		<div class="ip-header__logo">
			<a href="/">**internal / pointers</a>
		</div>
		<div class="ip-header__links">
			<ul>
				<li><a href="/rss">rss</a></li>
				<li><a href="/about">about</a></li>
			</ul>
		</div>
	</div>
</div>

<div class="ip-sub-header">
</div>
		<div class="ip-body">

	<div class="ip-container">

		<div class="ip-post">

			<div class="ip-post__info">
				<p>— Written by Triangles on October 29, 2017 
								• updated on November 10, 2019  
								• ID 59 —</p>
			</div>

			<div class="ip-post__title">
				<h1>The cost function in logistic regression</h1>
			</div>

			<div class="ip-post__intro">
				<p>Preparing the logistic regression algorithm for the actual implementation.</p>
			</div>

						<div class="ip-post__other-box">
				<div class="ip-post__other-box__section-title">Other articles from this series</div>

				<ul class="ip-post__other-box__post-list">
														<li>
						<p>
							<span class="title">
								<a href="/post/introduction-machine-learning">Introduction to machine learning</a>
							</span> —
							<span class="intro">What machine learning is about, types of learning and classification algorithms, introductory examples.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/linear-regression-one-variable">Linear regression with one variable</a>
							</span> —
							<span class="intro">Finding the best-fitting straight line through points of a data set.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/gradient-descent-function">The gradient descent function</a>
							</span> —
							<span class="intro">How to find the minimum of a function using an iterative algorithm.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/gradient-descent-action">The gradient descent in action</a>
							</span> —
							<span class="intro">It's time to put together the gradient descent with the cost function, in order to churn out the final algorithm for linear regression.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/multivariate-linear-regression">Multivariate linear regression</a>
							</span> —
							<span class="intro">How to upgrade a linear regression algorithm from one to many input variables.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/optimize-gradient-descent-algorithm">How to optimize the gradient descent algorithm</a>
							</span> —
							<span class="intro">A collection of practical tips and tricks to improve the gradient descent process and make it easier to understand.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/introduction-classification-and-logistic-regression">Introduction to classification and logistic regression</a>
							</span> —
							<span class="intro">Get your feet wet with another fundamental machine learning algorithm for binary classification.</span>
						</p>
					</li>
																												<li>
						<p>
							<span class="title">
								<a href="/post/problem-overfitting-machine-learning-algorithms">The problem of overfitting in machine learning algorithms</a>
							</span> —
							<span class="intro">Overfitting makes linear regression and logistic regression perform poorly. A technique called "regularization" aims to fix the problem for good.</span>
						</p>
					</li>
													</ul>
			</div>
			
			<div class="ip-post__body">
				<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- internalpointers responsive -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1778432007040046"
     data-ad-slot="1269254897"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<p>In the previous article "<a href="https://www.internalpointers.com/post/introduction-classification-and-logistic-regression">Introduction to classification and logistic regression</a>" I outlined the mathematical basics of the logistic regression algorithm, whose task is to <em>separate</em> things in the training example by computing the decision boundary.</p>
<p>The decision boundary can be described by an equation. As in linear regression, the logistic regression algorithm will be able to find the best [texi]\theta[texi]s parameters in order to make the decision boundary actually separate the data points correctly. In this article we'll see how to compute those [texi]\theta[texi]s.</p>
<p>Suppose we have a generic training set</p>
<p>[tex]\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(m)}, y^{(m)}) \}[tex]</p>
<p>made of [texi]m[texi] training examples, where <span>[texi](x^{(1)}, y^{(1)})[texi]</span> is the 1st example and so on. More specifically, <span>[texi]x^{(m)}[texi]</span> is the input variable of the [texi]m[texi]-th example, while <span>[texi]y^{(m)}[texi]</span> is its output variable. Being this a classification problem, each example has of course the output [texi]y[texi] bound between [texi]0[texi] and [texi]1[texi]. In other words, [texi]y \in {0,1}[texi].</p>
<p>Each example is represented as usual by its feature vector</p>
<p><span>
[tex]
\vec{x} = 
\begin{bmatrix}
  x_0 \\ x_1 \\ \dots \\ x_n 
\end{bmatrix}
[tex]
</span></p>
<p>where [texi]x_0 = 1[texi] (the <a href="https://www.internalpointers.com/post/multivariate-linear-regression">same old trick</a>). This is a generic example, we don't know the exact number of features.</p>
<p>Finally we have the hypothesis function for logistic regression, as seen in the previous article:</p>
<p><span>
[tex]
h_\theta(x) = \frac{1}{1 + e^{\theta^{\top} x}}
[tex]
</span></p>
<p>Our task now is to choose the best parameters [texi]\theta[texi]s in the equation above, given the current training set, in order to minimize errors. Remember that [texi]\theta[texi] is not a single parameter: it expands to the equation of the decision boundary which can be a line or a more complex formula (with more [texi]\theta[texi]s to guess).</p>
<p>The procedure is similar to what we did for linear regression: define a cost function and try to find the best possible values of each [texi]\theta[texi] by minimizing the cost function output. The minimization will be performed by a gradient descent algorithm, whose task is to parse the cost function output until it finds the lowest minimum point.</p>
<h3>The cost function used in linear regression won't work here</h3>
<p>You might remember the original cost function [texi]J(\theta)[texi] used in <a href="https://www.internalpointers.com/post/linear-regression-one-variable">linear regression</a>. I can tell you right now that it's not going to work here with logistic regression. If you try to use the linear regression's cost function to generate [texi]J(\theta)[texi] in a logistic regression problem, you would end up with a <strong>non-convex</strong> function: a wierdly-shaped graph with no easy to find minimum global point, as seen in the picture below.</p>
<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/10/non-convex-function.svg" alt="Non-convex function">
<div class="caption">1. An example of a non-convex function. The grey point on the right side shows a potential local minimum.</div>
</div><p>This strange outcome is due to the fact that in logistic regression we have the sigmoid function around, which is non-linear (i.e. not a line). With the [texi]J(\theta)[texi] depicted in figure 1. the gradient descent algorithm might get stuck in a local minimum point. That's why we still need a neat <strong>convex</strong> function as we did for linear regression: a bowl-shaped function that eases the gradient descent function's work to converge to the optimal minimum point.</p>
<h2>A better cost function for logistic regression</h2>
<p>Let me go back for a minute to the cost function we used in linear regression:</p>
<p><span>
[tex]
J(\vec{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
[tex]
</span></p>
<p>which can be rewritten in a slightly different way:</p>
<p><span>
[tex]
J(\vec{\theta}) = \frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2
[tex]
</span></p>
<p>Nothing scary happened: I've just moved the [texi]\frac{1}{2}[texi] next to the summation part. Now let's make it more general by defining a new function</p>
<p><span>[tex]\mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) = \frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2[tex]</span></p>
<p>In words, a function [texi]\mathrm{Cost}[texi] that takes two parameters in input: <span>[texi]h_\theta(x^{(i)})[texi]</span> as hypothesis function and [texi]y^{(i)}[texi] as output. You can think of it as the cost the algorithm has to pay if it makes a prediction  [texi]h_\theta(x^{(i)})[texi] while the actual label was [texi]y^{(i)}[texi].</p>
<p>With this new piece of the puzzle I can rewrite the cost function for the linear regression as follows:</p>
<p><span>
[tex]
J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)})
[tex]
</span></p>
<p>However we know that the linear regression's cost function cannot be used in logistic regression problems. So what is this all about? Well, it turns out that for logistic regression we just have to find a different [texi]\mathrm{Cost}[texi] function, while the summation part stays the same.</p>
<h3>Logistic regression cost function</h3>
<p>For logistic regression, the [texi]\mathrm{Cost}[texi] function is defined as:</p>
<p><span>
[tex]
\mathrm{Cost}(h_\theta(x),y) =
\begin{cases}
-\log(h_\theta(x)) & \text{if y = 1} \\
-\log(1-h_\theta(x)) & \text{if y = 0}
\end{cases}
[tex]
</span></p>
<p>The [texi]i[texi] indexes have been removed for clarity. In words this is the cost the algorithm pays if it predicts a value 
[texi]h_\theta(x)[texi] while the actual cost label turns out to be [texi]y[texi]. By using this function we will grant the <em>convexity</em> to the function the gradient descent algorithm has to process, as discussed above. There is also a mathematical proof for that, which is outside the scope of this introductory course.</p>
<p>In case [texi]y = 1[texi], the output (i.e. the cost to pay) approaches to 0 as <span>[texi]h_\theta(x)[texi]</span> approaches to 1. Conversely, the cost to pay grows to infinity as <span>[texi]h_\theta(x)[texi]</span> approaches to 0. You can clearly see it in the plot 2. below, left side. This is a desirable property: we want a bigger penalty as the algorithm predicts something far away from the actual value. If the label is [texi]y = 1[texi] but the algorithm predicts [texi]h_\theta(x) = 0[texi], the outcome is completely wrong.</p>
<p>Conversely, the same intuition applies when [texi]y = 0[texi], depicted in the plot 2. below, right side. Bigger penalties when the label is [texi]y = 0[texi] but the algorithm predicts [texi]h_\theta(x) = 1[texi].</p>
<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/10/cost-function-logistic-regression.svg" alt="Cost function for logistic regression">
<div class="caption">2. How the cost function for logistic regression looks like.</div>
</div><h3>Additional cost function optimizations</h3>
<p>What we have just seen is the verbose version of the cost function for logistic regression. We can make it more compact into a one-line expression: this will help avoiding boring if/else statements when converting the formula into an algorithm.</p>
<p><span>
[tex]
\mathrm{Cost}(h_\theta(x),y) = -y \log(h_\theta(x)) - (1 - y) \log(1-h_\theta(x))
[tex]
</span></p>
<p>Proof: try to replace [texi]y[texi] with 0 and 1 and you will end up with the two pieces of the original function.</p>
<p>With the optimization in place, the logistic regression cost function can be rewritten as:</p>
<p><span>
[tex]
\begin{align}
J(\theta) & = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \\
& = - \dfrac{1}{m} [\sum_{i=1}^{m} y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1-h_\theta(x^{(i)}))] \\
\end{align}
[tex]
</span></p>
<p>I've moved the minus sign outside to avoid additional parentheses.</p>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- internalpointers responsive -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1778432007040046"
     data-ad-slot="1269254897"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h2>Plugging the cost function and the gradient descent together</h2>
<p>What's left? We have the hypothesis function and the cost function: we are almost done. It's now time to find the best values for [texi]\theta[texi]s parameters in the cost function, or in other words to minimize the cost function by running the gradient descent algorithm. The procedure is identical to what we did for linear regression.</p>
<p>More formally, we want to minimize the cost function:</p>
<p><span>
[tex]
\min_{\theta} J(\theta)
[tex]
</span></p>
<p>Which will output a set of parameters [texi]\theta[texi], the best ones (i.e. with less error). Once done, we will be ready to make predictions on <em>new</em> input examples with their features [texi]x[texi], by using the new [texi]\theta[texi]s in the hypothesis function:</p>
<p><span>
[tex]
h_\theta(x) = \frac{1}{1 + e^{\theta^{\top} x}}
[tex]
</span></p>
<p>Where [texi]h_\theta(x)[texi] is the output, the prediction, or yet the probability that [texi]y = 1[texi].</p>
<p>The way we are going to minimize the cost function is by using the gradient descent. The good news is that the procedure is 99% identical to <a href="https://www.internalpointers.com/post/gradient-descent-action">what we did for linear regression</a>.</p>
<p>To minimize the cost function we have to run the gradient descent function on each parameter:</p>
<p><span>
[tex]
\begin{align} 
\text{repeat until convergence \{} \\
\theta_j & := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\ 
\text{\}}
\end{align}
[tex]
</span></p>
<p>Remember to simultaneously update all [texi]\theta_j[texi] as we did in the linear regression counterpart: if you have [texi]
n[texi] features, that is a feature vector [texi]\vec{\theta} = [\theta_0, \theta_1, \cdots \theta_n][texi], all those parameters have to be updated simultaneously on each iteration:</p>
<p><span>
[tex]
\begin{align} 
\text{repeat until convergence \{} \\
\theta_0 & := \cdots \\ 
\theta_1 & := \cdots \\ 
\cdots \\ 
\theta_n & := \cdots \\ 
\text{\}}
\end{align}
[tex]
</span></p>
<p>Back to the algorithm, I'll spare you the computation of the daunting derivative [texi]\frac{\partial}{\partial \theta_j} J(\theta)[texi], which becomes:</p>
<p><span>
[tex]
\frac{\partial}{\partial \theta_j} J(\theta) = \dfrac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
[tex]
</span></p>
<p>So the loop above can be rewritten as:</p>
<p><span>
[tex]
\begin{align} 
\text{repeat until convergence \{} \\
\theta_j & := \theta_j - \alpha \dfrac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\ 
\text{\}}
\end{align}
[tex]
</span></p>
<p>Surprisingly, it looks identical to what we were doing for <a href="https://www.internalpointers.com/post/multivariate-linear-regression">the multivariate linear regression</a>. What's changed however is the definition of the hypothesis <span>[texi]h_\theta(x)[texi]</span>: for linear regression we had <span>[texi]h_\theta(x) = \theta^{\top}{x}[texi]</span>, whereas for logistic regression we have <span>[texi]h_\theta(x) = \frac{1}{1 + e^{\theta^{\top} x}}[texi]</span>.</p>
<p>From now on you can apply the same techniques to <a href="https://www.internalpointers.com/post/optimize-gradient-descent-algorithm">optimize the gradient descent algorithm</a> we have seen for linear regression, to make sure the conversion to the minimum point works correctly. In the next chapter I will delve into some advanced optimization tricks, as well as defining and avoiding the problem of <strong>overfitting</strong>.</p>
<h2>Sources</h2>
<p>Machine Learning Course @ Coursera - <em>Cost function</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function">video</a>)<br>
Machine Learning Course @ Coursera - <em>Simplified Cost Function and Gradient Descent</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent">video</a>)</p>			</div>


			<div class="ip-post__tags">
								<a class="ip-tag" href="/tag/machine-learning">machine learning</a>
				 • 								<a class="ip-tag" href="/tag/logistic-regression">logistic regression</a>
				 • 								<a class="ip-tag" href="/tag/classification">classification</a>
				 • 								<a class="ip-tag" href="/tag/hypothesis-function">hypothesis function</a>
				 • 								<a class="ip-tag" href="/tag/cost-function">cost function</a>
											</div>

			<div class="ip-post__neighbor-posts">
								<div class="ip-post__neighbor-posts__prev">
					<div class="ip-post__neighbor-posts__prev__label">
						previous article
					</div>
					<div class="ip-post__neighbor-posts__prev__title">          
						<a href="/post/introduction-classification-and-logistic-regression">Introduction to classification and logistic regression</a>
					</div>
				</div>
								
								<div class="ip-post__neighbor-posts__next">
					<div class="ip-post__neighbor-posts__next__label">
						next article
					</div>
					<div class="ip-post__neighbor-posts__next__title">          
						<a href="/post/problem-overfitting-machine-learning-algorithms">The problem of overfitting in machine learning algorithms</a>
					</div>
				</div>
							</div>
			
						<div class="ip-post__comments">
				<div class="ip-post__comments__title">
					comments
				</div>
				
								<div class="ip-post__comments__list">
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Moose</span> on 
							<span class="date">February 08, 2019 at 05:21</span>
						</div> 
						<div class="body">Could you please write the hypothesis function with the different theta's described like you did with multivariable linear regression: <br />
hθ(x)=θ0+θ1x1+θ2x2+...+θnxn<br />
<br />
What does it look like for logistical regression?</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Denson George</span> on 
							<span class="date">March 05, 2019 at 13:03</span>
						</div> 
						<div class="body">"There is also a mathematical proof for that, which is outside the scope of this introductory course."<br />
Triangles,do you have a post/another source where I can read up on this? Can you link it to the post for curious people like me.<br />
<br />
Many Thanks,<br />
Denson</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Triangles</span> on 
							<span class="date">March 08, 2019 at 10:35</span>
						</div> 
						<div class="body">@George my last-minute search led me to this: https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convex <br />
Not exactly a formal mathematical paper, It could be a start though.</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Salem</span> on 
							<span class="date">June 26, 2019 at 12:46</span>
						</div> 
						<div class="body">I have suggested a new algorithm to find the global optimum solution for nonlinear functions</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">yalamuri sreenivasulu reddy</span> on 
							<span class="date">July 03, 2019 at 14:44</span>
						</div> 
						<div class="body">hypothesis function for logistic regression is wrong it suppose to be h(theta) = 1/(1+e^(-theta'*x))</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Praveen</span> on 
							<span class="date">August 10, 2019 at 16:02</span>
						</div> 
						<div class="body">Very clear explanation, Thank you</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Carl</span> on 
							<span class="date">August 11, 2019 at 17:34</span>
						</div> 
						<div class="body">Do you know of a similar tutorial that is considering multiple classes than this binary case? I.e. using softmax expressions.</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Triangles</span> on 
							<span class="date">August 14, 2019 at 12:23</span>
						</div> 
						<div class="body">@Carl Not yet, sorry!</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">hasan</span> on 
							<span class="date">September 03, 2019 at 14:42</span>
						</div> 
						<div class="body">You are missing a minus sign in the exponent in the hypothesis function of the logistic regression. The correct form should be:<br />
$h_\theta(x) = \frac{1}{1 + e^{-\theta^{\top} x}}$</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">mahesh</span> on 
							<span class="date">April 16, 2020 at 16:01</span>
						</div> 
						<div class="body">Nice explanation. how does thetas learned using maximum likehood estimation</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Zeeshan</span> on 
							<span class="date">May 04, 2020 at 21:34</span>
						</div> 
						<div class="body">In the last formula for cost function, the Summation sign should be outside the square bracket</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Bharpur Singh</span> on 
							<span class="date">May 07, 2020 at 09:06</span>
						</div> 
						<div class="body">The logistic or Sigmoid function is written wrongly it should be negative of theta transpose x.</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Roman</span> on 
							<span class="date">August 17, 2020 at 07:27</span>
						</div> 
						<div class="body">How do we jump from linear J to logistic J = -ylog(g(x)) - ylog(1-g(x)) ?</div> 
					</div>
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">dinesh</span> on 
							<span class="date">March 04, 2021 at 16:27</span>
						</div> 
						<div class="body">why the cost function changes  for logistic why not the gradient descent function ?<br />
</div> 
					</div>
									</div>
							</div>
			
			<div class="ip-post__social-tools">
				<div class="ip-post__social-tools__title">
					share!
				</div>
				<div class="ip-post__social-tools__twitter item"></div>
			</div>

		</div>

	</div>

</div>

		<div class="ip-footer">
	<div class="ip-container">
		© 2015-2024 — Monocasual Laboratories — 
		<a href="/tos" rel="nofollow">terms of service</a> — 
		<a href="/privacy" rel="nofollow">privacy policy</a> — 
		<a href="/about">about</a> — 
		<a href="/rss">RSS feed</a>
	</div>
</div>
	</body>
</html>

