<!DOCTYPE html>
<html lang="en">
	<head>
		 
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23296419-22"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-23296419-22');
		</script>
		 

		<title>How to optimize the gradient descent algorithm - Internal Pointers</title>

		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Monocasual Laboratories">
		<meta name="description" content="A collection of practical tips and tricks to improve the gradient descent process and make it easier to understand.">
		<meta name="keywords" content="machine learning,linear regression,gradient descent,min-max scaling,normalization,z-score">
		<meta name="copyright" content="2015-2024 Monocasual Laboratories">
		<meta name="application-name" content="Internal Pointers">
		<meta name="google-site-verification" content="d6wzhBnnEXNHg7kty5SNXVBKd4e29wUFP69SROd-3eI" />

		<meta property="og:title" content="How to optimize the gradient descent algorithm" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.internalpointers.com/post/optimize-gradient-descent-algorithm" />
<meta property="og:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="900" />
<meta property="og:site_name" content="Internal Pointers" />
<meta property="og:description" content="A collection of practical tips and tricks to improve the gradient descent process and make it easier to understand." />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://www.internalpointers.com/post/optimize-gradient-descent-algorithm" />
<meta name="twitter:title" content="How to optimize the gradient descent algorithm" />
<meta name="twitter:description" content="A collection of practical tips and tricks to improve the gradient descent process and make it easier to understand." />
<meta name="twitter:image" content="https://www.internalpointers.com/img/internalpointers-card.png" />

		<link rel="icon" href="/img/favicon.ico">
		<link rel="apple-touch-icon-precomposed" href="/img/favicon-152.png">
		<link rel="stylesheet" href="/main-1.4.0.css">

				
		<script defer src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
		<script defer src="//cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js"></script>
		<script defer src="/main-1.4.0.js"></script>

		

<script defer type="text/x-mathjax-config">
	MathJax.Hub.Config({
		displayAlign: 'center',
		asciimath2jax: {
			delimiters: [['§','§']]
		},
		tex2jax: {
			inlineMath: [['[texi]','[texi]']],
			displayMath: [['[tex]','[tex]']]
		}
	});
</script>
<script defer type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
	</head>
	<body>
		<div class="ip-follow-us-popup">

    <div class="ip-follow-us-popup__side">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
    </div>

    <div class="ip-follow-us-popup__header">
        <p>Join us on Facebook!</p>
    </div>

    <div class="ip-follow-us-popup__body">
        <div class="ip-follow-us-popup__body__ok">
        <img src="/img/facebook-like-thumb.svg" alt="Like it!">
        </div>
    </div>

    <div class="ip-follow-us-popup__footer">
        <div><a href="#" class="ip-follow-us-popup__footer__nope">Nope, thanks anyway.</a></div>
    </div>

</div>		<div class="ip-cookie-banner">
    <div class="ip-container">
        <p>We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. By using our site, you acknowledge that you have read and understand our <a href="{{ url('/privacy/') }}">Privacy Policy</a>, and our <a href="{{ url('/tos/') }}">Terms of Service</a>. Your use of this site is subject to these policies and terms. | <a href="#" class="ip-cookie-banner__close">ok, got it</a></p>
    </div>
</div>		<div class="ip-header">
	<div class="ip-container">
		<div class="ip-header__logo">
			<a href="/">**internal / pointers</a>
		</div>
		<div class="ip-header__links">
			<ul>
				<li><a href="/rss">rss</a></li>
				<li><a href="/about">about</a></li>
			</ul>
		</div>
	</div>
</div>

<div class="ip-sub-header">
</div>
		<div class="ip-body">

	<div class="ip-container">

		<div class="ip-post">

			<div class="ip-post__info">
				<p>— Written by Triangles on April 01, 2017 
								• ID 52 —</p>
			</div>

			<div class="ip-post__title">
				<h1>How to optimize the gradient descent algorithm</h1>
			</div>

			<div class="ip-post__intro">
				<p>A collection of practical tips and tricks to improve the gradient descent process and make it easier to understand.</p>
			</div>

						<div class="ip-post__other-box">
				<div class="ip-post__other-box__section-title">Other articles from this series</div>

				<ul class="ip-post__other-box__post-list">
														<li>
						<p>
							<span class="title">
								<a href="/post/introduction-machine-learning.html">Introduction to machine learning</a>
							</span> —
							<span class="intro">What machine learning is about, types of learning and classification algorithms, introductory examples.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/linear-regression-one-variable.html">Linear regression with one variable</a>
							</span> —
							<span class="intro">Finding the best-fitting straight line through points of a data set.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/gradient-descent-function.html">The gradient descent function</a>
							</span> —
							<span class="intro">How to find the minimum of a function using an iterative algorithm.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/gradient-descent-action.html">The gradient descent in action</a>
							</span> —
							<span class="intro">It's time to put together the gradient descent with the cost function, in order to churn out the final algorithm for linear regression.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/multivariate-linear-regression.html">Multivariate linear regression</a>
							</span> —
							<span class="intro">How to upgrade a linear regression algorithm from one to many input variables.</span>
						</p>
					</li>
																												<li>
						<p>
							<span class="title">
								<a href="/post/introduction-classification-and-logistic-regression.html">Introduction to classification and logistic regression</a>
							</span> —
							<span class="intro">Get your feet wet with another fundamental machine learning algorithm for binary classification.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/cost-function-logistic-regression.html">The cost function in logistic regression</a>
							</span> —
							<span class="intro">Preparing the logistic regression algorithm for the actual implementation.</span>
						</p>
					</li>
																			<li>
						<p>
							<span class="title">
								<a href="/post/problem-overfitting-machine-learning-algorithms.html">The problem of overfitting in machine learning algorithms</a>
							</span> —
							<span class="intro">Overfitting makes linear regression and logistic regression perform poorly. A technique called "regularization" aims to fix the problem for good.</span>
						</p>
					</li>
													</ul>
			</div>
			
			<div class="ip-post__body">
				<p>Real-world data can come up in different orders of magnitude. For example, your age ranges from 0 to 100 years, while your yearly income from €10,000 to €10,000,000 (and more). Using such unprocessed data as input features for a linear regression system might slow down the gradient descent algorithm to a crawl.</p>
<p>It happens because — as we will see shortly — such not normalized data warps the cost function the gradient descent has to process, making the minimum point really difficult to reach.</p>
<p>Because of that, an important trick in machine learning and in linear regression is to make sure that all the input features are <em>on a similar scale</em>. This is a preparatory step you do in order to optimize the input data, known as <strong>feature scaling</strong>.</p>
<h2>Feature scaling</h2>
<p>In feature scaling you basically normalize your input values. For example, say you have two features:</p>
<ul>
<li>[texi]x_1[texi] as the yearly income (10,000-10,000,000);</li>
<li>[texi]x_2[texi] as the age (0-100).</li>
</ul>
<p>Below you will find a contour plot for the cost function [texi]J(\theta_1, \theta_2)[texi] as if we were using the raw, unprocessed values. As you may see the result is a very thin and stretched version of it. The gradient descent algorithm would oscillate a lot back and forth, taking a long time before finding its way to the minimum point.</p>
<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/03/contour-plot-thin.png" alt="Stretched version of the contour plot.">
<div class="caption">1. A stretched contour plot, due to missing input feature scaling.</div>
</div><p>With feature scaling we will bring back the original bowl-shaped figure in order to let the gradient descent algorithm do its job efficiently. You have to options here: <strong>min-max scaling</strong> or <strong>standardization</strong>.</p>
<h3>Min-max scaling</h3>
<p>The idea is to get every input feature into approximately a [texi][-1,1][texi] range. The name comes from the use of [texi]\min[texi] and [texi]\max[texi] functions, namely the smallest and greatest values in your dataset. It requires dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable:</p>
<p><span>
[tex]
x_i^{\prime} = \frac{x_i - \min(x_i)}{\max(x_i) - \min(x_i)}
[tex]
</span></p>
<p>where [texi]x_i[texi] is the original [texi]i[texi]-th input value, [texi]x_i^{\prime}[texi] is the normalized version.</p>
<p>For example, say I'm dealing with the yearly income [texi]x_1[texi] and in particular I want to normalize the value of $30,000:</p>
<p><span>
[tex]
x_1^{\prime} = \frac{30,000 - 10,000}{10,000,000 - 10,000} \approx 0.002
[tex]
</span></p>
<p>Just rinse and repeat such normalization for every value in your dataset. Of course if you are in a multivariate scenario remember to skip feature [texi]x_0[texi], since [texi]x_0 = 1[texi] as seen in the <a href="https://www.internalpointers.com/post/multivariate-linear-regression">previous episode</a>.</p>
<h3>Standardization</h3>
<p>This technique goes also under the name of <strong>z-score normalization</strong> and many other confusing aliases I wish I could forget. In brief, you transform your data set so that the values follow the property of a <a href="https://www.internalpointers.com/post/normal-distribution">normal distribution</a>, namely with <a href="https://www.internalpointers.com/post/averages-mean-median-mode-range">mean</a> 0 ([texi]\mu = 0[texi]) and <a href="https://www.internalpointers.com/post/standard-deviation">standard deviation</a> 1 ([texi]\sigma = 1[texi]). Unlike min-max scaling, with standardization you are thinking in terms of how many standard deviations a value is far from the mean of the entire data set.</p>
<p>The general formula for standardization:</p>
<p><span>
[tex]
x_i = \frac{x_i - \mu_i}{\sigma_i}
[tex]
</span></p>
<p>Following the links to my previous articles above I'm able to compute the mean and the standard deviation on my data set. I'll show you an example with the yearly income [texi]x_1[texi]. Suppose I have collected five samples: $10,000, $30,000, $32,000, $35,000, $150,000. The mean and the standard deviation are:</p>
<p><span>
[tex]
\begin{equation}
\mu_1 = 51,400 
\qquad
\sigma_1 \approx 50,078
\end{equation}
[tex]
</span></p>
<p>Now, let's apply the standardization to the value of $30,000 as I did before with the min-max scaling:</p>
<p><span>
[tex]
x_1 = \frac{30,000 - 51,400}{50,078} \approx -0.4
[tex]
</span></p>
<p>You can read it as [texi]-0.4[texi] standard deviations ([texi]-0.4\text{STD}[texi]) from the mean.</p>
<p>Rinse and repeat the procedure for every value in your dataset as for the min-max scaling, and remember to skip [texi]x_0[texi] in multivariate problems.</p>
<p>Using standardization is important when you are comparing measurements that have different units, like years and dollars. It is also a general requirement for many machine learning algorithms besides linear regression. As a rule of thumb I'd say: when in doubt, just standardize the data, it shouldn't hurt.</p>
<h2>Debug the gradient descent to make sure it is working properly</h2>
<p>You want to know if the gradient descent is working correctly. Since the job of the gradient descent is to find the value of [texi]\theta[texi]s that minimize the cost function, you could plot the cost function itself (i.e. its output) and see how it behaves as the algorithm runs.</p>
<p>The image below shows what I mean. The number of iterations on the horizontal axis, the cost function output on the vertical one. On each iteration the  gradient descent churns out new [texi]\theta[texi]s values: you take those values and evaluate the cost function [texi]J(\theta)[texi]. You should see a descending curve if the algorithm behaves well: it means that it's minimizing the value of [texi]\theta[texi]s correctly.</p>
<p>More generally, the gradient descent works properly when [texi]J(\theta)[texi] decreases after every iteration.</p>
<div class="ip-img">
<img src="https://s3.amazonaws.com/images.internalpointers.com/2017/03/cost-function-plot.png" alt="Cost function plot.">
<div class="caption">2. Plot of the cost function as it gets minimized by the gradient descent algorithm.</div>
</div><p>Plotting [texi]J(\theta)[texi] also tells you whether or not the gradient descent has converged. Different problems require different number of iterations until convergence, so in general you can assume that the algorithm has found a minimum when [texi]J(\theta)[texi] decreases less than some small value [texi]\epsilon[texi] in one iteration.</p>
<p>Choosing a proper value [texi]\epsilon[texi] is not an easy task. Some people set it to value [texi]10^-3[texi] and also automatize the task in what is called <strong>automatic convergence test</strong>: their algorithm stops when [texi]J(\theta)[texi] has decreased less than [texi]\epsilon[texi] in one iteration.</p>
<h2>Choose the best values for [texi]\alpha[texi]</h2>
<p>If your [texi]J(\theta)[texi] plot seen before starts to look weird — upward curves, dramatically slow decreasing, ... — the gradient descent is not working properly: it is time to fix [texi]\alpha[texi], by using a <em>smaller</em> value.</p>
<p>It has been proved mathematically that for sufficiently small [texi]\alpha[texi], [texi]J(\theta)[texi] decreases on every iteration. On the other hand if [texi]\alpha[texi] is too small the gradient descent can be slow to converge.</p>
<p>The rule of thumb here is to try a range of [texi]\alpha[texi] values. Start with [texi]\alpha = 0,001[texi] and look at the [texi]J(\theta)[texi] plot. Does it decrease properly and rapidly? You are done with it. Otherwise, switch to [texi]\alpha = 0,01[texi] ([texi]\times 10[texi] scale), rinse and repeat until the algorithm works fine.</p>
<h2>Sources</h2>
<p>Machine Learning @ Coursera - <em>Gradient Descent in Practice I - Feature Scaling</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling">link</a>)<br>
Machine Learning @ Coursera - <em>Gradient Descent in Practice II - Learning Rate</em> (<a href="https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate">link</a>)<br>
Wikipedia - <em>Feature Scaling</em> (<a href="https://en.wikipedia.org/wiki/Feature_scaling">link</a>)<br>
Sebastianraschka.com - <em>About Feature Scaling and Normalization</em> (<a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html">link</a>)</p>			</div>


			<div class="ip-post__tags">
								<a class="ip-tag" href="/tag/machine-learning">machine learning</a>
				 • 								<a class="ip-tag" href="/tag/linear-regression">linear regression</a>
				 • 								<a class="ip-tag" href="/tag/gradient-descent">gradient descent</a>
				 • 								<a class="ip-tag" href="/tag/min-max-scaling">min-max scaling</a>
				 • 								<a class="ip-tag" href="/tag/normalization">normalization</a>
				 • 								<a class="ip-tag" href="/tag/z-score">z-score</a>
											</div>

			<div class="ip-post__neighbor-posts">
								<div class="ip-post__neighbor-posts__prev">
					<div class="ip-post__neighbor-posts__prev__label">
						previous article
					</div>
					<div class="ip-post__neighbor-posts__prev__title">          
						<a href="/post/multivariate-linear-regression.html">Multivariate linear regression</a>
					</div>
				</div>
								
								<div class="ip-post__neighbor-posts__next">
					<div class="ip-post__neighbor-posts__next__label">
						next article
					</div>
					<div class="ip-post__neighbor-posts__next__title">          
						<a href="/post/introduction-classification-and-logistic-regression.html">Introduction to classification and logistic regression</a>
					</div>
				</div>
							</div>
			
						<div class="ip-post__comments">
				<div class="ip-post__comments__title">
					comments
				</div>
				
								<div class="ip-post__comments__list">
										<div class="ip-post__comments__list__comment">
						<div class="info">
							<span class="author">Kevin McLaughlin</span> on 
							<span class="date">May 02, 2019 at 20:04</span>
						</div> 
						<div class="body">I love the article. The one thing I would ask is, how do you transform the parameters found in the normalized domain back to the original space. I know how to transform the observations and z-scores back and for, but what about the parameters?</div> 
					</div>
									</div>
							</div>
			
			<div class="ip-post__social-tools">
				<div class="ip-post__social-tools__title">
					share!
				</div>
				<div class="ip-post__social-tools__twitter item"></div>
			</div>

		</div>

	</div>

</div>

		<div class="ip-footer">
	<div class="ip-container">
		© 2015-2024 — Monocasual Laboratories — 
		<a href="/tos" rel="nofollow">terms of service</a> — 
		<a href="/privacy" rel="nofollow">privacy policy</a> — 
		<a href="/about">about</a> — 
		<a href="/rss">RSS feed</a>
	</div>
</div>
	</body>
</html>

